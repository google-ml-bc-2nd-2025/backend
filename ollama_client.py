import os
import json
import re
from typing import Dict, TypedDict, List, Annotated, Sequence, Literal, Optional, Union
# dotenv ì„í¬íŠ¸ ì¶”ê°€
from dotenv import load_dotenv
# ìƒˆë¡œìš´ ì„í¬íŠ¸ ì‚¬ìš©
from langchain_ollama import OllamaLLM
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

PORT = 11434
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ MODEL ê°’ ê°€ì ¸ì˜¤ê¸°
DEFAULT_MODEL = os.getenv("MODEL", "gemma3:4b")  # ê¸°ë³¸ê°’ì€ "gemma3:4b"

# ìƒíƒœ íƒ€ì… ì •ì˜
class AgentState(TypedDict):
    question: str
    thoughts: List[str]
    research_results: str
    answer: str
    is_game_resource_request: bool
    resource_type: Optional[str]
    resource_details: Dict[str, str]
    work_results: Optional[str]
    next: str

def create_ollama_llm(model_name=DEFAULT_MODEL, streaming=False):
    """LangChain Ollama LLM ìƒì„±"""
    callbacks = [StreamingStdOutCallbackHandler()] if streaming else []
    return OllamaLLM(
        model=model_name,
        base_url=f"http://localhost:{PORT}",
        callbacks=callbacks
    )

def check_game_resource_request(state: AgentState) -> AgentState:
    """ê²Œì„ ë¦¬ì†ŒìŠ¤ ì œì‘ ìš”ì²­ì¸ì§€ í™•ì¸"""
    llm = create_ollama_llm()
    prompt = PromptTemplate.from_template(
        """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ê²Œì„ ë¦¬ì†ŒìŠ¤(3D ëª¨ë¸ ë˜ëŠ” ì• ë‹ˆë©”ì´ì…˜) ì œì‘ ìš”ì²­ì¸ì§€ ë¶„ì„í•˜ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        
        ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        1. is_game_resource_request: ê²Œì„ ë¦¬ì†ŒìŠ¤ ì œì‘ ìš”ì²­ì¸ì§€ ì—¬ë¶€ (true/false)
        2. resource_type: ìš”ì²­ëœ ë¦¬ì†ŒìŠ¤ ìœ í˜• ("3d_model", "animation", "other" ì¤‘ í•˜ë‚˜)
        3. ìš”ì²­ëœ ë¦¬ì†ŒìŠ¤ì˜ ì„¸ë¶€ ì •ë³´ (ìºë¦­í„° ì´ë¦„, ìŠ¤íƒ€ì¼, í¬ì¦ˆ ë“±)
        
        JSON í˜•ì‹ ì‘ë‹µ:"""
    )
    
    analysis = llm.invoke(prompt.format(question=state["question"]))
    
    try:
        json_match = re.search(r'(\{.*\})', analysis, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            analysis_dict = json.loads(json_str)
        else:
            analysis_dict = json.loads(analysis)
    except:
        analysis_dict = {
            "is_game_resource_request": False,
            "resource_type": "other",
            "details": {}
        }
    
    is_valid_request = (analysis_dict.get("is_game_resource_request", False) and 
                        analysis_dict.get("resource_type") in ["3d_model", "animation"])
    
    next_step = "think" if is_valid_request else "reject_request"
    
    return {
        **state,
        "is_game_resource_request": analysis_dict.get("is_game_resource_request", False),
        "resource_type": analysis_dict.get("resource_type", "other"),
        "resource_details": analysis_dict.get("details", {}),
        "next": next_step
    }

def reject_request(state: AgentState) -> AgentState:
    """ê²Œì„ ë¦¬ì†ŒìŠ¤ ìš”ì²­ì´ ì•„ë‹Œ ê²½ìš° ê±°ë¶€ ë©”ì‹œì§€ ìƒì„±"""
    return {
        **state,
        "answer": "ì§€ê¸ˆì€ 3D ëª¨ë¸ê³¼ ì• ë‹ˆë©”ì´ì…˜ ì œì‘ ìš”ì²­ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "next": END
    }

def think(state: AgentState) -> AgentState:
    """ì‚¬ê³  ë‹¨ê³„: ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì ‘ê·¼ ë°©ë²• ê²°ì •"""
    llm = create_ollama_llm()
    prompt = PromptTemplate.from_template(
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì• ë‹ˆë©”ì´ì…˜ ì œì‘ì— ëŒ€í•œ ì‚¬ê³  ê³¼ì •ì„ ì„¤ëª…í•˜ì„¸ìš”.:
        
        ì§ˆë¬¸: {question}
        
        ì‚¬ê³  ê³¼ì •:"""
    )
    
    thoughts = llm.invoke(prompt.format(question=state["question"]))
    
    return {
        **state,
        "thoughts": state.get("thoughts", []) + [thoughts],
        "next": "research_step"
    }

def research(state: AgentState) -> AgentState:
    """ì—°êµ¬ ë‹¨ê³„: ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ ìˆ˜ì§‘"""
    llm = create_ollama_llm()
    
    if state.get("is_game_resource_request", False):
        prompt = PromptTemplate.from_template(
            """ì‚¬ìš©ìê°€ ê²Œì„ ë¦¬ì†ŒìŠ¤ ì œì‘ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œì‘ ë°©ë²•ê³¼ ë‹¨ê³„ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”:
            
            ìš”ì²­ ìœ í˜•: {resource_type}
            ìš”ì²­ ì„¸ë¶€ ì •ë³´: {resource_details}
            
            ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. í•„ìš”í•œ ì†Œí”„íŠ¸ì›¨ì–´ì™€ ë„êµ¬
            2. ì œì‘ ë‹¨ê³„ì™€ í”„ë¡œì„¸ìŠ¤
            3. ì¼ë°˜ì ì¸ ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­
            4. ì‘ì—… ì‹œê°„ ì¶”ì •
            
            ìì„¸í•œ ì¡°ì‚¬ ê²°ê³¼:"""
        )
        
        resource_type = "3D ëª¨ë¸" if state["resource_type"] == "3d_model" else "ì• ë‹ˆë©”ì´ì…˜"
        research_results = llm.invoke(
            prompt.format(
                resource_type=resource_type,
                resource_details=json.dumps(state["resource_details"], ensure_ascii=False)
            )
        )
    else:
        prompt = PromptTemplate.from_template(
            """ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¡°ì‚¬í•˜ê³  ìˆ˜ì§‘í•˜ì„¸ìš”:
            
            ì§ˆë¬¸: {question}
            ì§€ê¸ˆê¹Œì§€ ì‚¬ê³ : {thoughts}
            
            ì¡°ì‚¬ ê²°ê³¼:"""
        )
        
        research_results = llm.invoke(
            prompt.format(
                question=state["question"],
                thoughts="\n".join(state["thoughts"])
            )
        )
    
    return {
        **state,
        "research_results": research_results,
        "next": "work_step" if state.get("is_game_resource_request", False) else "answer_step"
    }

def work_step(state: AgentState) -> AgentState:
    """ì‘ì—… ë‹¨ê³„: ì‹¤ì œ ê²Œì„ ë¦¬ì†ŒìŠ¤ ìƒì„± ì‘ì—… ìˆ˜í–‰"""
    resource_type = state.get("resource_type", "other")
    
    if resource_type == "3d_model":
        work_results = """
        [3D ëª¨ë¸ ìƒì„± ê²°ê³¼]
        - íŒŒì¼ í˜•ì‹: FBX, OBJ, GLTF
        - í´ë¦¬ê³¤ ìˆ˜: 15,420
        - í…ìŠ¤ì²˜ ë§µ: Diffuse, Normal, Roughness, Metallic
        - ë¦¬ê¹…: ì™„ë£Œ
        - ë¯¸ë¦¬ë³´ê¸° URL: https://example.com/preview/model123.jpg
        - ë‹¤ìš´ë¡œë“œ URL: https://example.com/download/model123.zip
        
        ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ URLì—ì„œ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
    elif resource_type == "animation":
        work_results = """
        [ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ê²°ê³¼]
        - íŒŒì¼ í˜•ì‹: FBX, BVH
        - í”„ë ˆì„ ìˆ˜: 120
        - ì• ë‹ˆë©”ì´ì…˜ ê¸¸ì´: 4ì´ˆ
        - ì• ë‹ˆë©”ì´ì…˜ ìœ í˜•: ê±·ê¸°/ë‹¬ë¦¬ê¸° ì‚¬ì´í´
        - ë¯¸ë¦¬ë³´ê¸° URL: https://example.com/preview/anim123.gif
        - ë‹¤ìš´ë¡œë“œ URL: https://example.com/download/anim123.zip
        
        ì• ë‹ˆë©”ì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ URLì—ì„œ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
    else:
        work_results = "ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¦¬ì†ŒìŠ¤ ìœ í˜•ì…ë‹ˆë‹¤."
    
    return {
        **state,
        "work_results": work_results,
        "next": "answer_step"
    }

def generate_answer(state: AgentState) -> AgentState:
    """ë‹µë³€ ìƒì„± ë‹¨ê³„: ìµœì¢… ë‹µë³€ ì‘ì„±"""
    llm = create_ollama_llm()
    
    if state.get("is_game_resource_request", False) and state.get("work_results"):
        prompt = PromptTemplate.from_template(
            """ê²Œì„ ë¦¬ì†ŒìŠ¤ ì œì‘ ìš”ì²­ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
            
            ìš”ì²­ ë‚´ìš©: {question}
            ìš”ì²­ ìœ í˜•: {resource_type}
            ì¡°ì‚¬ ê²°ê³¼: {research_results}
            ì‘ì—… ê²°ê³¼: {work_results}
            
            ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë¦¬ì†ŒìŠ¤ ì œì‘ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:"""
        )
        
        resource_type = "3D ëª¨ë¸" if state["resource_type"] == "3d_model" else "ì• ë‹ˆë©”ì´ì…˜"
        answer = llm.invoke(
            prompt.format(
                question=state["question"],
                resource_type=resource_type,
                research_results=state["research_results"],
                work_results=state["work_results"]
            )
        )
    else:
        prompt = PromptTemplate.from_template(
            """ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:
            
            ì§ˆë¬¸: {question}
            ì§€ê¸ˆê¹Œì§€ ì‚¬ê³ : {thoughts}
            ì¡°ì‚¬ ê²°ê³¼: {research_results}
            
            ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ìµœì¢… ë‹µë³€:"""
        )
        
        answer = llm.invoke(
            prompt.format(
                question=state["question"],
                thoughts="\n".join(state["thoughts"]),
                research_results=state["research_results"]
            )
        )
    
    return {
        **state,
        "answer": answer,
        "next": END
    }

def router(state: AgentState) -> Union[Literal["think", "research_step", "answer_step", "work_step", "reject_request"], Literal[END]]:
    """ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    return state["next"]

def build_agent_graph():
    """ì—ì´ì „íŠ¸ ê·¸ë˜í”„ êµ¬ì„±"""
    workflow = StateGraph(AgentState)
    
    workflow.add_node("check_game_resource", check_game_resource_request)
    workflow.add_node("reject_request", reject_request)
    workflow.add_node("think", think)
    workflow.add_node("research_step", research)
    workflow.add_node("work_step", work_step)
    workflow.add_node("answer_step", generate_answer)
    
    workflow.set_entry_point("check_game_resource")
    
    workflow.add_conditional_edges("check_game_resource", router, {
        "think": "think",
        "reject_request": "reject_request"
    })
    
    workflow.add_conditional_edges("reject_request", router, {
        END: END
    })
    
    workflow.add_conditional_edges("think", router, {
        "research_step": "research_step",
        "answer_step": "answer_step"
    })
    
    workflow.add_conditional_edges("research_step", router, {
        "work_step": "work_step",
        "answer_step": "answer_step"
    })
    
    workflow.add_conditional_edges("work_step", router, {
        "answer_step": "answer_step"
    })
    
    workflow.add_conditional_edges("answer_step", router, {
        END: END
    })
    
    return workflow.compile()

def answer_with_agent(question: str, model_name=DEFAULT_MODEL):
    """LangGraph ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€"""
    agent = build_agent_graph()
    
    initial_state = {
        "question": question,
        "thoughts": [],
        "research_results": "",
        "answer": "",
        "is_game_resource_request": False,
        "resource_type": None,
        "resource_details": {},
        "work_results": None,
        "next": "check_game_resource"
    }
    
    result = agent.invoke(initial_state)
    return result

def streaming_agent_execution(question: str, model_name=DEFAULT_MODEL):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥"""
    print("=== ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ ===\n")
    
    print(f"ğŸ“ ì§ˆë¬¸: {question}\n")
    
    agent = build_agent_graph()
    
    initial_state = {
        "question": question,
        "thoughts": [],
        "research_results": "",
        "answer": "",
        "is_game_resource_request": False,
        "resource_type": None,
        "resource_details": {},
        "work_results": None,
        "next": "check_game_resource"
    }
    
    for event in agent.stream(initial_state):
        node = event.get("node")
        if node:
            state = event["state"]
            
            if node == "check_game_resource":
                is_valid = state.get("is_game_resource_request", False) and state.get("resource_type") in ["3d_model", "animation"]
                print("\nğŸ” ìš”ì²­ ë¶„ì„ ê²°ê³¼:")
                print(f"ê²Œì„ ë¦¬ì†ŒìŠ¤ ìš”ì²­: {'ì˜ˆ' if state.get('is_game_resource_request', False) else 'ì•„ë‹ˆì˜¤'}")
                if state.get('is_game_resource_request', False):
                    print(f"ë¦¬ì†ŒìŠ¤ ìœ í˜•: {state.get('resource_type', 'ì—†ìŒ')}")
                    print(f"ìœ íš¨í•œ ìš”ì²­: {'ì˜ˆ' if is_valid else 'ì•„ë‹ˆì˜¤'}")
            
            elif node == "reject_request":
                print("\nâŒ ìš”ì²­ ê±°ë¶€:")
                print(state["answer"])
            
            elif node == "think" and state.get("thoughts"):
                print("\nğŸ§  ì‚¬ê³  ê²°ê³¼:")
                print(state["thoughts"][-1])
                print("\nğŸ” ì¡°ì‚¬ ë‹¨ê³„ ì‹œì‘...")
            
            elif node == "research_step" and state.get("research_results"):
                print("\nğŸ” ì¡°ì‚¬ ê²°ê³¼:")
                print(state["research_results"])
                
                if state.get("is_game_resource_request", False):
                    print("\nğŸ”¨ ë¦¬ì†ŒìŠ¤ ìƒì„± ì‘ì—… ì‹œì‘...")
                else:
                    print("\nâœï¸ ë‹µë³€ ìƒì„± ì‹œì‘...")
            
            elif node == "work_step" and state.get("work_results"):
                print("\nğŸ”¨ ë¦¬ì†ŒìŠ¤ ìƒì„± ê²°ê³¼:")
                print(state["work_results"])
                print("\nâœï¸ ë‹µë³€ ìƒì„± ì‹œì‘...")
            
            elif node == "answer_step" and state.get("answer"):
                print("\nâœ… ìµœì¢… ë‹µë³€:")
                print(state["answer"])
    
    print("\n=== ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ ===")

def generate_with_gemma3(prompt, model=DEFAULT_MODEL, stream=False):
    """
    Gemma ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        prompt (str): ëª¨ë¸ì— ì „ì†¡í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
        model (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: gemma3:4b)
        stream (bool): ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€
        
    Returns:
        dict: ëª¨ë¸ì˜ ì‘ë‹µ ê²°ê³¼
    """
    try:
        if stream:
            result = answer_with_agent(prompt, model_name=model)
            return {
                "response": result["answer"],
                "done": True
            }
        else:
            result = answer_with_agent(prompt, model_name=model)
            return {
                "response": result["answer"],
                "done": True
            }
    except Exception as e:
        return {"error": f"ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

if __name__ == "__main__":
    print("=== LangGraphë¡œ Ollama ì—ì´ì „íŠ¸ ì‚¬ìš©í•˜ê¸° ===\n")
    
    question1 = "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë„ˆë¦¬ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    print(f"ì§ˆë¬¸: {question1}")
    print("\n--- ì—ì´ì „íŠ¸ ë‹µë³€ ---")
    result = answer_with_agent(question1)
    print(f"ìµœì¢… ë‹µë³€: {result['answer']}")
    
    print("\n\n=== ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰ ===")
    question2 = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ 3ê°€ì§€ ê´€ì ì—ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    streaming_agent_execution(question2)